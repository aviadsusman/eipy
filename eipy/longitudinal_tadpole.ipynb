{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import eipy.ei as e\n",
    "from eipy.additional_ensembles import MeanAggregation, CES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eipy.metrics import fmax_score\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef\n",
    "\n",
    "metrics = {\n",
    "            'f_max': fmax_score,\n",
    "            'auc': roc_auc_score,\n",
    "            'mcc': matthews_corrcoef\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictors = {\n",
    "                    'ADAB': AdaBoostClassifier(),\n",
    "                    'XGB': XGBClassifier(),\n",
    "                    'DT': DecisionTreeClassifier(),\n",
    "                    'RF': RandomForestClassifier(),\n",
    "                    'GB': GradientBoostingClassifier(),\n",
    "                    'KNN': KNeighborsClassifier(),\n",
    "                    'LR': LogisticRegression(),\n",
    "                    'NB': GaussianNB(),\n",
    "                    'MLP': MLPClassifier(),\n",
    "                    'SVM': SVC(probability=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"/home/opc/eipy/tadpole/tadpole_data_time_imptn_norm_v1.pickle\", \"rb\") as file:\n",
    "    data = pkl.load(file)\n",
    "with open(\"/home/opc/eipy/tadpole/tadpole_labels_time_imptn_norm_v1.pickle\", \"rb\") as file:\n",
    "    labels = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data[k] for k in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in labels.items():\n",
    "    labels[k] = v.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate transformation to make sure labels are ordered correctly in time\n",
    "labels = pd.DataFrame(labels)\n",
    "\n",
    "labels = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "encoding_dict = {'NL': 0, 'MCI': 1, 'Dementia': 2}\n",
    "\n",
    "# Use numpy.vectorize with a lambda function to apply the encoding\n",
    "labels = np.vectorize(lambda x: encoding_dict[x])(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    502\n",
      "0    200\n",
      "Name: count, dtype: int64\n",
      "1    480\n",
      "0    195\n",
      "2     27\n",
      "Name: count, dtype: int64\n",
      "1    451\n",
      "0    192\n",
      "2     59\n",
      "Name: count, dtype: int64\n",
      "1    378\n",
      "0    188\n",
      "2    136\n",
      "Name: count, dtype: int64\n",
      "1    348\n",
      "2    178\n",
      "0    176\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(pd.Series(labels[:,i]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''misalign data'''\n",
    "data = data[1:]\n",
    "labels = labels[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, (702, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating metadata for timestep 0\n",
      "Training base predictors on Main cognitive tests...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI volumes...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on Demo, APOE4 & others...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (WM Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (Cortical Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Surface Area...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Cortical Thickness Average...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Cortical Thickness Standard Deviation...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "generating metadata for timestep 1\n",
      "Training base predictors on Main cognitive tests...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI volumes...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on Demo, APOE4 & others...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (WM Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (Cortical Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Surface Area...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Cortical Thickness Average...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Cortical Thickness Standard Deviation...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "generating metadata for timestep 2\n",
      "Training base predictors on Main cognitive tests...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI volumes...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on Demo, APOE4 & others...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (WM Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (Cortical Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Surface Area...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Cortical Thickness Average...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Cortical Thickness Standard Deviation...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "generating metadata for timestep 3\n",
      "Training base predictors on Main cognitive tests...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI volumes...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on Demo, APOE4 & others...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (WM Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (Cortical Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Surface Area...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Cortical Thickness Average...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Cortical Thickness Standard Deviation...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_data = []\n",
    "for t in range(len(data)):\n",
    "    #time dependent data splitting\n",
    "    X_train_timestep = data[t]\n",
    "    labels_at_timestep = labels[:, t]\n",
    "    EI_for_timestep = e.EnsembleIntegration(\n",
    "                        base_predictors=base_predictors,\n",
    "                        k_outer=5,\n",
    "                        k_inner=5,\n",
    "                        n_samples=1,\n",
    "                        sampling_strategy=None,\n",
    "                        sampling_aggregation=\"mean\",\n",
    "                        n_jobs=-1,\n",
    "                        metrics=metrics,\n",
    "                        random_state=38,\n",
    "                        project_name=f\"time step {t}\",\n",
    "                        model_building=False,\n",
    "                        )\n",
    "    print(f\"generating metadata for timestep {t}\")\n",
    "    EI_for_timestep.fit_base(X_train_timestep, labels_at_timestep)\n",
    "    meta_data.append([EI_for_timestep.ensemble_training_data, EI_for_timestep.ensemble_test_data, EI_for_timestep.ensemble_training_data_final, EI_for_timestep.base_summary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_training_data = [[dfs[0][i] for dfs in meta_data] for i in range(5)]\n",
    "RNN_test_data = [[dfs[1][i] for dfs in meta_data] for i in range(5)]\n",
    "RNN_training_data_final = [df[2] for df in meta_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make first time point in meta-data multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(frame):\n",
    "    column_names = []\n",
    "    for i in range(frame.columns.nlevels):\n",
    "        if i == 0:\n",
    "            column_names.append(frame.columns.get_level_values(i).unique().drop(\"labels\"))\n",
    "            \n",
    "        else:\n",
    "            column_names.append(frame.columns.get_level_values(i).unique().drop(''))\n",
    "    \n",
    "    return column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_first_time_point(df):\n",
    "    new_columns = get_column_names(df)\n",
    "    classes=[0,1,2]\n",
    "    new_columns.append(classes)\n",
    "    new_mux=pd.MultiIndex.from_product(iterables=new_columns, names=[\"modality\", \"base predictor\", \"sample\", \"class\"])\n",
    "    new_df = pd.DataFrame(columns=new_mux)\n",
    "\n",
    "    for col in new_df.columns:\n",
    "        if col[-1] == 0:\n",
    "            new_df[col] = 1 - df[col[:-1]]\n",
    "        elif col[-1] == 1:\n",
    "            new_df[col] = df[col[:-1]]\n",
    "        else:\n",
    "            new_df[col] = 0\n",
    "    \n",
    "    new_df['labels'] = df['labels']\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n",
      "/tmp/ipykernel_2787885/2424389332.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['labels'] = df['labels']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(RNN_training_data)):\n",
    "    RNN_training_data[i][0] = fix_first_time_point(RNN_training_data[i][0])\n",
    "    RNN_test_data[i][0] = fix_first_time_point(RNN_test_data[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 23:34:51.115840: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-11 23:34:51.143893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-11 23:34:51.143918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-11 23:34:51.144685: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-11 23:34:51.149231: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-11 23:34:51.149728: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 23:34:51.941426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def ordinal_regression_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Ordinal regression loss function.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: true labels (ground truth)\n",
    "    - y_pred: predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - loss value\n",
    "    \"\"\"\n",
    "    # Assuming y_true and y_pred are tensors of shape (batch_size, num_classes)\n",
    "\n",
    "    # Calculate cumulative probabilities for true and predicted labels\n",
    "    true_cum_probs = K.cumsum(K.softmax(y_true, axis=-1), axis=-1)\n",
    "    pred_cum_probs = K.cumsum(K.softmax(y_pred, axis=-1), axis=-1)\n",
    "\n",
    "    # Calculate the ordinal regression loss\n",
    "    loss = K.sum((true_cum_probs - pred_cum_probs) ** 2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2787885/1450835205.py:14: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  RNN_training_data_fold = [data.drop(columns=[\"labels\"], axis=1) for data in RNN_training_data[i]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561, 4, 240) (561, 4, 3)\n",
      "18/18 [==============================] - 1s 3ms/step - loss: 0.8824 - accuracy: 0.6417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2787885/1450835205.py:21: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  RNN_test_data_fold = [data.drop(columns=[\"labels\"], axis=1) for data in RNN_test_data[i]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "(561, 4, 240) (561, 4, 3)\n",
      "18/18 [==============================] - 1s 3ms/step - loss: 0.8753 - accuracy: 0.6315\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "(562, 4, 240) (562, 4, 3)\n",
      "18/18 [==============================] - 1s 3ms/step - loss: 0.8684 - accuracy: 0.6232\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "(562, 4, 240) (562, 4, 3)\n",
      "18/18 [==============================] - 1s 3ms/step - loss: 0.8960 - accuracy: 0.6165\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "(562, 4, 240) (562, 4, 3)\n",
      "18/18 [==============================] - 1s 3ms/step - loss: 0.9184 - accuracy: 0.6117\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_preds = [] # LSTM predictions at every time point. Will be populated by 5 arrays\n",
    "for i in range(len(RNN_training_data)):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM,Dense\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units=50+10*i, input_shape=(4,240), return_sequences=True))\n",
    "    lstm.add(Dense(units=3, activation='softmax'))\n",
    "    lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #loss='categorical_crossentropy'\n",
    "\n",
    "\n",
    "    labels_across_time = np.column_stack([df['labels'].values for df in RNN_training_data[i]])\n",
    "    labels_across_time = np.eye(3)[labels_across_time]\n",
    "\n",
    "    RNN_training_data_fold = [data.drop(columns=[\"labels\"], axis=1) for data in RNN_training_data[i]]\n",
    "    data_arrays_per_timepoint = [df.to_numpy() for df in RNN_training_data_fold]\n",
    "    tensor_3d = np.stack(data_arrays_per_timepoint, axis=0)\n",
    "    tensor_3d = np.transpose(tensor_3d, (1,0,2))\n",
    "    print(tensor_3d.shape, labels_across_time.shape)\n",
    "    lstm.fit(tensor_3d, labels_across_time)\n",
    "\n",
    "    RNN_test_data_fold = [data.drop(columns=[\"labels\"], axis=1) for data in RNN_test_data[i]]\n",
    "    data_arrays_per_timepoint_test = [df.to_numpy() for df in RNN_test_data_fold]\n",
    "    tensor_3d_test = np.stack(data_arrays_per_timepoint_test, axis=0)\n",
    "    tensor_3d_test = np.transpose(tensor_3d_test, (1,0,2))\n",
    "\n",
    "    y_preds.append(lstm.predict(tensor_3d_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_argmax = [np.argmax(pred, axis=-1) for pred in y_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "for i in range(len(RNN_test_data)):\n",
    "    y_true = pd.concat([data[\"labels\"] for data in RNN_test_data[i]], axis=1).to_numpy()\n",
    "    y_trues.append(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "aucs=[]\n",
    "for i in range(len(y_preds)):\n",
    "    aucs_for_fold=[]\n",
    "    for j in range(1, y_preds[i].shape[1]):\n",
    "        aucs_for_fold.append(roc_auc_score(y_trues[i][:,j], y_preds[i][:,j], multi_class='ovr'))\n",
    "    aucs.append(aucs_for_fold)\n",
    "\n",
    "auc_df = pd.DataFrame(data=aucs, columns=[\"m06\", \"m12\", \"m24\", \"m36\"])\n",
    "auc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1s=[]\n",
    "for i in range(len(y_preds_argmax)):\n",
    "    f1s_for_fold=[]\n",
    "    for j in range(y_preds_argmax[i].shape[-1]):\n",
    "        f1s_for_fold.append(f1_score(y_preds_argmax[i][:,j], y_trues[i][:,j], average='micro'))\n",
    "    f1s.append(f1s_for_fold)\n",
    "\n",
    "f1_df = pd.DataFrame(data=f1s, columns=[\"bl\", \"m06\", \"m12\", \"m24\", \"m36\"])\n",
    "f1_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# confusion_matrices = [ [confusion_matrix(y_pred=y_preds_argmax[i][:,j], y_true=y_trues[i][:,j]) for j in range(5)] for i in range(len(y_trues))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/opc/eipy/eipy/longitudinal_tadpole.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baviad_bc/home/opc/eipy/eipy/longitudinal_tadpole.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m classification_reports \u001b[39m=\u001b[39m [[classification_report(y_pred\u001b[39m=\u001b[39my_preds_argmax[i][:,j], y_true\u001b[39m=\u001b[39my_trues[i][:,j]) \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y_trues))]\n",
      "\u001b[1;32m/home/opc/eipy/eipy/longitudinal_tadpole.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baviad_bc/home/opc/eipy/eipy/longitudinal_tadpole.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m classification_reports \u001b[39m=\u001b[39m [[classification_report(y_pred\u001b[39m=\u001b[39my_preds_argmax[i][:,j], y_true\u001b[39m=\u001b[39my_trues[i][:,j]) \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y_trues))]\n",
      "\u001b[1;32m/home/opc/eipy/eipy/longitudinal_tadpole.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baviad_bc/home/opc/eipy/eipy/longitudinal_tadpole.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m classification_reports \u001b[39m=\u001b[39m [[classification_report(y_pred\u001b[39m=\u001b[39my_preds_argmax[i][:,j], y_true\u001b[39m=\u001b[39my_trues[i][:,j]) \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y_trues))]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "classification_reports = [[classification_report(y_pred=y_preds_argmax[i][:,j], y_true=y_trues[i][:,j]) for j in range(5)] for i in range(len(y_trues))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " FOLD 1 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        40\n",
      "           1       0.72      1.00      0.83       101\n",
      "\n",
      "    accuracy                           0.72       141\n",
      "   macro avg       0.36      0.50      0.42       141\n",
      "weighted avg       0.51      0.72      0.60       141\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.68      1.00      0.81        96\n",
      "           2       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.68       141\n",
      "   macro avg       0.23      0.33      0.27       141\n",
      "weighted avg       0.46      0.68      0.55       141\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.64      1.00      0.78        90\n",
      "           2       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.64       141\n",
      "   macro avg       0.21      0.33      0.26       141\n",
      "weighted avg       0.41      0.64      0.50       141\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.54      1.00      0.70        76\n",
      "           2       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.54       141\n",
      "   macro avg       0.18      0.33      0.23       141\n",
      "weighted avg       0.29      0.54      0.38       141\n",
      "\n",
      " \n",
      " FOLD 2 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        40\n",
      "           1       0.72      1.00      0.83       101\n",
      "\n",
      "    accuracy                           0.72       141\n",
      "   macro avg       0.36      0.50      0.42       141\n",
      "weighted avg       0.51      0.72      0.60       141\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.03      0.05        39\n",
      "           1       0.68      0.99      0.81        96\n",
      "           2       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.68       141\n",
      "   macro avg       0.39      0.34      0.29       141\n",
      "weighted avg       0.60      0.68      0.56       141\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.64      1.00      0.78        90\n",
      "           2       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.64       141\n",
      "   macro avg       0.21      0.33      0.26       141\n",
      "weighted avg       0.41      0.64      0.50       141\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.03      0.05        38\n",
      "           1       0.55      0.99      0.70        76\n",
      "           2       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.54       141\n",
      "   macro avg       0.27      0.34      0.25       141\n",
      "weighted avg       0.36      0.54      0.39       141\n",
      "\n",
      " \n",
      " FOLD 3 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        40\n",
      "           1       0.71      0.99      0.83       100\n",
      "\n",
      "    accuracy                           0.71       140\n",
      "   macro avg       0.36      0.49      0.41       140\n",
      "weighted avg       0.51      0.71      0.59       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.69      1.00      0.81        96\n",
      "           2       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.69       140\n",
      "   macro avg       0.23      0.33      0.27       140\n",
      "weighted avg       0.47      0.69      0.56       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.65      0.99      0.78        91\n",
      "           2       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64       140\n",
      "   macro avg       0.22      0.33      0.26       140\n",
      "weighted avg       0.42      0.64      0.51       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.03      0.05        38\n",
      "           1       0.54      0.97      0.70        75\n",
      "           2       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.53       140\n",
      "   macro avg       0.25      0.33      0.25       140\n",
      "weighted avg       0.34      0.53      0.39       140\n",
      "\n",
      " \n",
      " FOLD 4 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        40\n",
      "           1       0.71      1.00      0.83       100\n",
      "\n",
      "    accuracy                           0.71       140\n",
      "   macro avg       0.36      0.50      0.42       140\n",
      "weighted avg       0.51      0.71      0.60       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.69      1.00      0.81        96\n",
      "           2       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.69       140\n",
      "   macro avg       0.23      0.33      0.27       140\n",
      "weighted avg       0.47      0.69      0.56       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.64      1.00      0.78        90\n",
      "           2       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.64       140\n",
      "   macro avg       0.21      0.33      0.26       140\n",
      "weighted avg       0.41      0.64      0.50       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.05        37\n",
      "           1       0.54      1.00      0.70        75\n",
      "           2       0.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.54       140\n",
      "   macro avg       0.51      0.34      0.25       140\n",
      "weighted avg       0.55      0.54      0.39       140\n",
      "\n",
      " \n",
      " FOLD 5 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        40\n",
      "           1       0.71      1.00      0.83       100\n",
      "\n",
      "    accuracy                           0.71       140\n",
      "   macro avg       0.36      0.50      0.42       140\n",
      "weighted avg       0.51      0.71      0.60       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.69      1.00      0.81        96\n",
      "           2       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.69       140\n",
      "   macro avg       0.23      0.33      0.27       140\n",
      "weighted avg       0.47      0.69      0.56       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.64      1.00      0.78        90\n",
      "           2       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.64       140\n",
      "   macro avg       0.21      0.33      0.26       140\n",
      "weighted avg       0.41      0.64      0.50       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.54      1.00      0.70        76\n",
      "           2       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.54       140\n",
      "   macro avg       0.18      0.33      0.23       140\n",
      "weighted avg       0.29      0.54      0.38       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_preds_argmax)):\n",
    "    print(f\" \\n FOLD {i+1} \\n\")\n",
    "    for j in range(4):\n",
    "        print(classification_report(y_pred=y_preds_argmax[i][:,j], y_true=y_trues[i][:,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
