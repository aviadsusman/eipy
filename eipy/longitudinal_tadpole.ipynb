{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import eipy.ei as e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eipy.metrics import fmax_score\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef\n",
    "\n",
    "metrics = {\n",
    "            'f_max': fmax_score,\n",
    "            'auc': roc_auc_score,\n",
    "            'mcc': matthews_corrcoef\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictors = {\n",
    "                    'ADAB': AdaBoostClassifier(),\n",
    "                    'XGB': XGBClassifier(),\n",
    "                    'DT': DecisionTreeClassifier(),\n",
    "                    'RF': RandomForestClassifier(),\n",
    "                    'GB': GradientBoostingClassifier(),\n",
    "                    'KNN': KNeighborsClassifier(),\n",
    "                    'LR': LogisticRegression(),\n",
    "                    'NB': GaussianNB(),\n",
    "                    'MLP': MLPClassifier(),\n",
    "                    'SVM': SVC(probability=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"/home/opc/eipy/tadpole/tadpole_data_time_imptn_norm_v1.pickle\", \"rb\") as file:\n",
    "    data = pkl.load(file)\n",
    "with open(\"/home/opc/eipy/tadpole/tadpole_labels_time_imptn_norm_v1.pickle\", \"rb\") as file:\n",
    "    labels = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data[k] for k in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in labels.items():\n",
    "    labels[k] = v.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate transformation to make sure labels are ordered correctly in time\n",
    "labels = pd.DataFrame(labels)\n",
    "\n",
    "labels = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for multiclass version of data\n",
    "encoding_dict = {'NL': 0, 'MCI': 1, 'Dementia': 2}\n",
    "\n",
    "labels = np.vectorize(lambda x: encoding_dict[x])(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prevalence of each label across time\n",
    "for i in range(5):\n",
    "    print(pd.Series(labels[:,i]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''misalign data'''\n",
    "data = data[1:]\n",
    "labels = labels[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = []\n",
    "for t in range(len(data)):\n",
    "    #time dependent data splitting\n",
    "    X_train_test_timestep = data[t]\n",
    "    labels_at_timestep = labels[:, t]\n",
    "    EI_for_timestep = e.EnsembleIntegration(\n",
    "                        base_predictors=base_predictors,\n",
    "                        k_outer=5,\n",
    "                        k_inner=5,\n",
    "                        n_samples=1,\n",
    "                        sampling_strategy=None,\n",
    "                        sampling_aggregation=\"mean\",\n",
    "                        n_jobs=-1,\n",
    "                        metrics=metrics,\n",
    "                        random_state=38,\n",
    "                        project_name=f\"time step {t}\",\n",
    "                        model_building=False,\n",
    "                        )\n",
    "    print(f\"generating metadata for timestep {t}\")\n",
    "    EI_for_timestep.fit_base(X_train_test_timestep, labels_at_timestep)\n",
    "    meta_data.append([EI_for_timestep.ensemble_training_data, EI_for_timestep.ensemble_test_data, EI_for_timestep.ensemble_training_data_final, EI_for_timestep.base_summary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_training_data = [[dfs[0][i] for dfs in meta_data] for i in range(5)]\n",
    "RNN_test_data = [[dfs[1][i] for dfs in meta_data] for i in range(5)]\n",
    "RNN_training_data_final = [df[2] for df in meta_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make first time point in meta-data multiclass (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(df):\n",
    "    column_names = []\n",
    "    for i in range(df.columns.nlevels):\n",
    "        if i == 0:\n",
    "            column_names.append(df.columns.get_level_values(i).unique().drop(\"labels\"))\n",
    "            \n",
    "        else:\n",
    "            column_names.append(df.columns.get_level_values(i).unique().drop(''))\n",
    "    \n",
    "    return column_names\n",
    "\n",
    "def fix_first_time_point(df):\n",
    "    new_columns = get_column_names(df)\n",
    "    classes=[0,1,2]\n",
    "    new_columns.append(classes)\n",
    "    new_mux=pd.MultiIndex.from_product(iterables=new_columns, names=[\"modality\", \"base predictor\", \"sample\", \"class\"])\n",
    "    new_df = pd.DataFrame(columns=new_mux)\n",
    "\n",
    "    for col in new_df.columns:\n",
    "        if col[-1] == 0:\n",
    "            new_df[col] = 1 - df[col[:-1]]\n",
    "        elif col[-1] == 1:\n",
    "            new_df[col] = df[col[:-1]]\n",
    "        else:\n",
    "            new_df[col] = 0\n",
    "    \n",
    "    new_df['labels'] = df['labels']\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(RNN_training_data)):\n",
    "    RNN_training_data[i][0] = fix_first_time_point(RNN_training_data[i][0])\n",
    "    RNN_test_data[i][0] = fix_first_time_point(RNN_test_data[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Alternate RNN loss function for ordinal labels'''\n",
    "from keras import backend as K\n",
    "\n",
    "def ordinal_regression_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Ordinal regression loss function.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Calculate cumulative probabilities for true and predicted labels\n",
    "    true_cum_probs = K.cumsum(K.softmax(y_true, axis=-1), axis=-1)\n",
    "    pred_cum_probs = K.cumsum(K.softmax(y_pred, axis=-1), axis=-1)\n",
    "\n",
    "    # Calculate the ordinal regression loss\n",
    "    loss = K.sum((true_cum_probs - pred_cum_probs) ** 2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = [] # LSTM predictions at every time point. Will be populated by 5 arrays\n",
    "for i in range(len(RNN_training_data)):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM,Dense\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units=50+10*i, input_shape=(4,240), return_sequences=True))\n",
    "    lstm.add(Dense(units=3, activation='softmax'))\n",
    "    lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #loss='categorical_crossentropy'\n",
    "\n",
    "    #reformat labels\n",
    "    labels_across_time = np.column_stack([df['labels'].values for df in RNN_training_data[i]])\n",
    "    labels_across_time = np.eye(3)[labels_across_time]\n",
    "    # reformat data\n",
    "    RNN_training_data_fold = [df.drop(columns=[\"labels\"], axis=1, level=0) for df in RNN_training_data[i]]\n",
    "    data_arrays_per_timepoint = [df.to_numpy() for df in RNN_training_data_fold]\n",
    "    tensor_3d = np.stack(data_arrays_per_timepoint, axis=1)\n",
    "\n",
    "    lstm.fit(tensor_3d, labels_across_time)\n",
    "\n",
    "    #reformat test data\n",
    "    RNN_test_data_fold = [data.drop(columns=[\"labels\"], axis=1, level=0) for data in RNN_test_data[i]]\n",
    "    data_arrays_per_timepoint_test = [df.to_numpy() for df in RNN_test_data_fold]\n",
    "    tensor_3d_test = np.stack(data_arrays_per_timepoint_test, axis=1)\n",
    "\n",
    "\n",
    "    y_preds.append(lstm.predict(tensor_3d_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_argmax = [np.argmax(pred, axis=-1) for pred in y_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "for i in range(len(RNN_test_data)):\n",
    "    y_true = pd.concat([data[\"labels\"] for data in RNN_test_data[i]], axis=1).to_numpy()\n",
    "    y_trues.append(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "for i in range(len(y_preds_argmax)):\n",
    "    print(f\" \\n FOLD {i+1} \\n\")\n",
    "    for j in range(4):\n",
    "        print(classification_report(y_pred=y_preds_argmax[i][:,j], y_true=y_trues[i][:,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get mean and standard deviation of dementia class f1 score across folds'''\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "dem_f1 = []\n",
    "for i in range(len(y_preds_argmax)):\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_pred=y_preds_argmax[i][:,-1], y_true=y_trues[i][:,-1])\n",
    "    dem_f1.append(f1[2])\n",
    "\n",
    "np.mean(dem_f1), np.std(dem_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
