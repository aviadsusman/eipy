{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import eipy.ei as e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eipy.metrics import fmax_score\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef\n",
    "\n",
    "metrics = {\n",
    "            'f_max': fmax_score,\n",
    "            'auc': roc_auc_score,\n",
    "            'mcc': matthews_corrcoef\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictors = {\n",
    "                    'ADAB': AdaBoostClassifier(),\n",
    "                    'XGB': XGBClassifier(),\n",
    "                    'DT': DecisionTreeClassifier(),\n",
    "                    'RF': RandomForestClassifier(),\n",
    "                    'GB': GradientBoostingClassifier(),\n",
    "                    'KNN': KNeighborsClassifier(),\n",
    "                    'LR': LogisticRegression(),\n",
    "                    'NB': GaussianNB(),\n",
    "                    'MLP': MLPClassifier(),\n",
    "                    'SVM': SVC(probability=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"/home/opc/eipy/tadpole/tadpole_data_time_imptn_norm_v1.pickle\", \"rb\") as file:\n",
    "    data = pkl.load(file)\n",
    "with open(\"/home/opc/eipy/tadpole/tadpole_labels_time_imptn_norm_v1.pickle\", \"rb\") as file:\n",
    "    labels = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data[k] for k in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in labels.items():\n",
    "    labels[k] = v.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intermediate transformation to make sure labels are ordered correctly in time\n",
    "labels = pd.DataFrame(labels)\n",
    "\n",
    "labels = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for multiclass version of data\n",
    "encoding_dict = {'NL': 0, 'MCI': 1, 'Dementia': 2}\n",
    "\n",
    "labels = np.vectorize(lambda x: encoding_dict[x])(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    502\n",
      "0    200\n",
      "Name: count, dtype: int64\n",
      "1    480\n",
      "0    195\n",
      "2     27\n",
      "Name: count, dtype: int64\n",
      "1    451\n",
      "0    192\n",
      "2     59\n",
      "Name: count, dtype: int64\n",
      "1    378\n",
      "0    188\n",
      "2    136\n",
      "Name: count, dtype: int64\n",
      "1    348\n",
      "2    178\n",
      "0    176\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#prevalence of each label across time\n",
    "for i in range(5):\n",
    "    print(pd.Series(labels[:,i]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''misalign data'''\n",
    "data = data[:-1]\n",
    "labels = labels[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating metadata for timestep 0\n",
      "Training base predictors on Main cognitive tests...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI volumes...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on Demo, APOE4 & others...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (WM Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Volume (Cortical Parcellation)...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training base predictors on MRI ROI: Surface Area...\n",
      "        \n",
      "... for ensemble performance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ensemble training data: |██████████|100%\n",
      "Generating ensemble test data: |██████████|100%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m      6\u001b[0m EI_for_timestep \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mEnsembleIntegration(\n\u001b[1;32m      7\u001b[0m                     base_predictors\u001b[38;5;241m=\u001b[39mbase_predictors,\n\u001b[1;32m      8\u001b[0m                     k_outer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m                     model_building\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                     )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerating metadata for timestep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mEI_for_timestep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_test_timestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_at_timestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m meta_data\u001b[38;5;241m.\u001b[39mappend([EI_for_timestep\u001b[38;5;241m.\u001b[39mensemble_training_data, EI_for_timestep\u001b[38;5;241m.\u001b[39mensemble_test_data, EI_for_timestep\u001b[38;5;241m.\u001b[39mensemble_training_data_final, EI_for_timestep\u001b[38;5;241m.\u001b[39mbase_summary])\n",
      "File \u001b[0;32m~/eipy/eipy/ei.py:243\u001b[0m, in \u001b[0;36mEnsembleIntegration.fit_base\u001b[0;34m(self, X, y, base_predictors, modality_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m modality_name, modality \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mTraining base predictors on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodality_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m... for ensemble performance analysis...\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    242\u001b[0m )\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_base\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_predictors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_predictors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodality_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodality_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mTraining base predictors on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodality_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m... for ensemble performance analysis...\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    253\u001b[0m )\n",
      "File \u001b[0;32m~/.venv/lib64/python3.9/site-packages/sklearn/utils/_testing.py:188\u001b[0m, in \u001b[0;36m_IgnoreWarnings.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    187\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory)\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eipy/eipy/ei.py:426\u001b[0m, in \u001b[0;36mEnsembleIntegration._fit_base\u001b[0;34m(self, X, y, base_predictors, modality_name)\u001b[0m\n\u001b[1;32m    413\u001b[0m ensemble_training_data_modality \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_base_inner(\n\u001b[1;32m    414\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    415\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m     modality_name\u001b[38;5;241m=\u001b[39mmodality_name,\n\u001b[1;32m    420\u001b[0m )\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble_training_data \u001b[38;5;241m=\u001b[39m append_modality(\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble_training_data, ensemble_training_data_modality\n\u001b[1;32m    424\u001b[0m )\n\u001b[0;32m--> 426\u001b[0m ensemble_test_data_modality \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_base_outer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv_outer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_outer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_predictors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_predictors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodality_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodality_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble_test_data \u001b[38;5;241m=\u001b[39m append_modality(\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble_test_data, ensemble_test_data_modality\n\u001b[1;32m    436\u001b[0m )  \u001b[38;5;66;03m# append data to dataframe\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# create a summary of base predictor performance\u001b[39;00m\n",
      "File \u001b[0;32m~/eipy/eipy/ei.py:556\u001b[0m, in \u001b[0;36mEnsembleIntegration._fit_base_outer\u001b[0;34m(self, X, y, cv_outer, base_predictors, modality_name, model_building)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# define joblib Parallel function\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Parallel(\n\u001b[1;32m    553\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_backend\n\u001b[1;32m    554\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m parallel:\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;66;03m# spawn job for each sample, outer_fold and model\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_predict_single_base_predictor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfold_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_fold_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_building\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_building\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_predictors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbar_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbar_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mouter_fold_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv_outer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_numbers_for_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_building:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.venv/lib64/python3.9/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib64/python3.9/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib64/python3.9/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "meta_data = []\n",
    "for t in range(len(data)):\n",
    "    #time dependent data splitting\n",
    "    X_train_test_timestep = data[t]\n",
    "    labels_at_timestep = labels[:, t]\n",
    "    EI_for_timestep = e.EnsembleIntegration(\n",
    "                        base_predictors=base_predictors,\n",
    "                        k_outer=5,\n",
    "                        k_inner=5,\n",
    "                        n_samples=1,\n",
    "                        sampling_strategy=None,\n",
    "                        sampling_aggregation=\"mean\",\n",
    "                        n_jobs=-1,\n",
    "                        metrics=metrics,\n",
    "                        random_state=38,\n",
    "                        project_name=f\"time step {t}\",\n",
    "                        model_building=False,\n",
    "                        )\n",
    "    print(f\"generating metadata for timestep {t}\")\n",
    "    EI_for_timestep.fit_base(X_train_test_timestep, labels_at_timestep)\n",
    "    meta_data.append([EI_for_timestep.ensemble_training_data, EI_for_timestep.ensemble_test_data, EI_for_timestep.ensemble_training_data_final, EI_for_timestep.base_summary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_training_data = [[dfs[0][i] for dfs in meta_data] for i in range(5)]\n",
    "RNN_test_data = [[dfs[1][i] for dfs in meta_data] for i in range(5)]\n",
    "RNN_training_data_final = [df[2] for df in meta_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make first time point in meta-data multiclass (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(df):\n",
    "    column_names = []\n",
    "    for i in range(df.columns.nlevels):\n",
    "        if i == 0:\n",
    "            column_names.append(df.columns.get_level_values(i).unique().drop(\"labels\"))\n",
    "            \n",
    "        else:\n",
    "            column_names.append(df.columns.get_level_values(i).unique().drop(''))\n",
    "    \n",
    "    return column_names\n",
    "\n",
    "def fix_first_time_point(df):\n",
    "    new_columns = get_column_names(df)\n",
    "    classes=[0,1,2]\n",
    "    new_columns.append(classes)\n",
    "    new_mux=pd.MultiIndex.from_product(iterables=new_columns, names=[\"modality\", \"base predictor\", \"sample\", \"class\"])\n",
    "    new_df = pd.DataFrame(columns=new_mux)\n",
    "\n",
    "    for col in new_df.columns:\n",
    "        if col[-1] == 0:\n",
    "            new_df[col] = 1 - df[col[:-1]]\n",
    "        elif col[-1] == 1:\n",
    "            new_df[col] = df[col[:-1]]\n",
    "        else:\n",
    "            new_df[col] = 0\n",
    "    \n",
    "    new_df['labels'] = df['labels']\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(RNN_training_data)):\n",
    "    RNN_training_data[i][0] = fix_first_time_point(RNN_training_data[i][0])\n",
    "    RNN_test_data[i][0] = fix_first_time_point(RNN_test_data[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Alternate RNN loss function for ordinal labels'''\n",
    "from keras import backend as K\n",
    "\n",
    "def ordinal_regression_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Ordinal regression loss function.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Calculate cumulative probabilities for true and predicted labels\n",
    "    true_cum_probs = K.cumsum(K.softmax(y_true, axis=-1), axis=-1)\n",
    "    pred_cum_probs = K.cumsum(K.softmax(y_pred, axis=-1), axis=-1)\n",
    "\n",
    "    # Calculate the ordinal regression loss\n",
    "    loss = K.sum((true_cum_probs - pred_cum_probs) ** 2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = [] # LSTM predictions at every time point. Will be populated by 5 arrays\n",
    "for i in range(len(RNN_training_data)):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM,Dense\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units=50+10*i, input_shape=(4,240), return_sequences=True))\n",
    "    lstm.add(Dense(units=3, activation='softmax'))\n",
    "    lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #loss='categorical_crossentropy'\n",
    "\n",
    "    #reformat labels\n",
    "    labels_across_time = np.column_stack([df['labels'].values for df in RNN_training_data[i]])\n",
    "    labels_across_time = np.eye(3)[labels_across_time]\n",
    "    # reformat data\n",
    "    RNN_training_data_fold = [df.drop(columns=[\"labels\"], axis=1, level=0) for df in RNN_training_data[i]]\n",
    "    data_arrays_per_timepoint = [df.to_numpy() for df in RNN_training_data_fold]\n",
    "    tensor_3d = np.stack(data_arrays_per_timepoint, axis=1)\n",
    "\n",
    "    lstm.fit(tensor_3d, labels_across_time)\n",
    "\n",
    "    #reformat test data\n",
    "    RNN_test_data_fold = [data.drop(columns=[\"labels\"], axis=1, level=0) for data in RNN_test_data[i]]\n",
    "    data_arrays_per_timepoint_test = [df.to_numpy() for df in RNN_test_data_fold]\n",
    "    tensor_3d_test = np.stack(data_arrays_per_timepoint_test, axis=1)\n",
    "\n",
    "\n",
    "    y_preds.append(lstm.predict(tensor_3d_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_argmax = [np.argmax(pred, axis=-1) for pred in y_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "for i in range(len(RNN_test_data)):\n",
    "    y_true = pd.concat([data[\"labels\"] for data in RNN_test_data[i]], axis=1).to_numpy()\n",
    "    y_trues.append(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "for i in range(len(y_preds_argmax)):\n",
    "    print(f\" \\n FOLD {i+1} \\n\")\n",
    "    for j in range(4):\n",
    "        print(classification_report(y_pred=y_preds_argmax[i][:,j], y_true=y_trues[i][:,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get mean and standard deviation of dementia class f1 score across folds'''\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "dem_f1 = []\n",
    "for i in range(len(y_preds_argmax)):\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_pred=y_preds_argmax[i][:,-1], y_true=y_trues[i][:,-1])\n",
    "    dem_f1.append(f1[2])\n",
    "\n",
    "np.mean(dem_f1), np.std(dem_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
